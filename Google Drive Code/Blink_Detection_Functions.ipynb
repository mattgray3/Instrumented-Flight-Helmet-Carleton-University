{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "KM5FCJ1dnV9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries\n",
        "import os # Saving of files\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np  # Numerical computing and array operations\n",
        "import seaborn as sns  # Statistical data visualization\n",
        "import matplotlib.pyplot as plt  # Graph plotting and visualization\n",
        "from scipy.signal import butter, filtfilt, find_peaks  # Signal processing and filtering\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Similarity measurement between features"
      ],
      "metadata": {
        "id": "-PYnvW7p6n_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering Functions"
      ],
      "metadata": {
        "id": "2uf0DJ-onb53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code that acts as a bandpass filter, used to remove cable noise and motion artifacts\n",
        "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "# Scale the signal between 1 and -1\n",
        "def scale_signal(signal):\n",
        "    min_signal, max_signal = signal.min(), signal.max()\n",
        "    return 2 * (signal - min_signal) / (max_signal - min_signal) - 1\n",
        "\n",
        "# Should be called simplify signal, removes any small noise and flattens the signal\n",
        "def reduce_noise(signal, threshold):\n",
        "    return np.where((np.abs(signal) < threshold) | (signal > 0), 0, signal)\n",
        "\n",
        "# Designed to remove outliers however it seems to set the outlier to 0 which creates weird double troughs in the data\n",
        "def remove_outliers(signal, threshold, outlier_lowerbound, outlier_upperbound):\n",
        "    Q1 = np.percentile(signal, outlier_lowerbound)\n",
        "    Q3 = np.percentile(signal, outlier_upperbound)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - threshold * IQR\n",
        "    upper_bound = Q3 + threshold * IQR\n",
        "\n",
        "    median_value = np.median(signal[(signal >= lower_bound) & (signal <= upper_bound)])\n",
        "    cleaned_signal = np.where((signal < lower_bound) | (signal > upper_bound), median_value, signal)\n",
        "\n",
        "    return cleaned_signal"
      ],
      "metadata": {
        "id": "eQL-VviSnZmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Functions"
      ],
      "metadata": {
        "id": "uYEda3GHnaBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6-n6yld0pQO"
      },
      "outputs": [],
      "source": [
        "# In order to accuratly caclulate the duration the stablepoints must be snapped to the cloest peak.\n",
        "def snap_stable_points_to_peaks(stable_points, bandpassed_signal, search_radius):\n",
        "    snapped_points = []\n",
        "    n = len(bandpassed_signal)\n",
        "\n",
        "    for (start_idx, end_idx) in stable_points:\n",
        "        start_left = max(0, start_idx - search_radius)\n",
        "        start_right = start_idx\n",
        "        if start_right - start_left > 0:\n",
        "            local_max_start = np.argmax(bandpassed_signal[start_left:start_right+1])\n",
        "            new_start_idx = start_left + local_max_start\n",
        "        else:\n",
        "            new_start_idx = start_idx\n",
        "        end_left = end_idx\n",
        "        end_right = min(n - 1, end_idx + search_radius)\n",
        "        if end_right - end_left > 0:\n",
        "            local_max_end = np.argmax(bandpassed_signal[end_left:end_right+1])\n",
        "            new_end_idx = end_left + local_max_end\n",
        "        else:\n",
        "            new_end_idx = end_idx\n",
        "\n",
        "        snapped_points.append((new_start_idx, new_end_idx))\n",
        "    return snapped_points\n",
        "\n",
        "# Finds all the troughs in the data, thier stable points and then searches for the peaks to snap the stable points too\n",
        "def identify_troughs_and_stable_points(signal, threshold, time_array, bandpassed_signal, search_radius, trough_proximity_threshold):\n",
        "    valid_troughs = []\n",
        "    stable_indices = []\n",
        "    amplitudes = []\n",
        "    valid_indices = []\n",
        "\n",
        "    troughs, _ = find_peaks(-signal, prominence=threshold)\n",
        "    troughs = troughs[signal[troughs] < 0]\n",
        "\n",
        "    for trough in troughs:\n",
        "        before_idx_arr = np.where(signal[:trough] >= 0)[0]\n",
        "        before_idx = before_idx_arr[-1] if len(before_idx_arr) > 0 else None\n",
        "\n",
        "        after_idx_arr = np.where(signal[trough:] >= 0)[0]\n",
        "        after_idx = after_idx_arr[0] + trough if len(after_idx_arr) > 0 else None\n",
        "\n",
        "        if before_idx is not None and after_idx is not None:\n",
        "            stable_indices.append((before_idx, after_idx))\n",
        "            valid_troughs.append(trough)\n",
        "\n",
        "    snapped_indices = snap_stable_points_to_peaks(stable_indices, bandpassed_signal, search_radius)\n",
        "\n",
        "    for trough, (snap_before, snap_after) in zip(valid_troughs, snapped_indices):\n",
        "        avg_stable = (bandpassed_signal[snap_before] + bandpassed_signal[snap_after]) / 2.0\n",
        "        trough_value = bandpassed_signal[trough]\n",
        "        amplitude = abs(avg_stable - trough_value)\n",
        "        amplitudes.append(amplitude)\n",
        "\n",
        "    trough_times = time_array[valid_troughs]\n",
        "\n",
        "    sorted_order = np.argsort(trough_times)\n",
        "    sorted_times = trough_times[sorted_order]\n",
        "    sorted_amplitudes = np.array(amplitudes)[sorted_order]\n",
        "    sorted_troughs = np.array(valid_troughs)[sorted_order]\n",
        "    sorted_snapped = [snapped_indices[i] for i in sorted_order]\n",
        "\n",
        "    i = 0\n",
        "    while i < len(sorted_times):\n",
        "        current_range = [i]\n",
        "        j = i + 1\n",
        "        while j < len(sorted_times) and (sorted_times[j] - sorted_times[i]) <= trough_proximity_threshold:\n",
        "            current_range.append(j)\n",
        "            j += 1\n",
        "        max_amp_idx = current_range[np.argmax(np.abs(sorted_amplitudes[current_range]))]\n",
        "        valid_indices.append(max_amp_idx)\n",
        "        i = j\n",
        "\n",
        "    valid_indices = np.array(valid_indices, dtype=int)\n",
        "    trough_times = sorted_times[valid_indices]\n",
        "    amplitudes = sorted_amplitudes[valid_indices]\n",
        "    valid_troughs = sorted_troughs[valid_indices]\n",
        "    snapped_indices = [sorted_snapped[i] for i in valid_indices]\n",
        "\n",
        "    snapped_time_points = [(time_array[start], time_array[end]) for start, end in snapped_indices]\n",
        "\n",
        "    return trough_times, snapped_time_points, amplitudes\n",
        "\n",
        "# This is what actually calcualtes the values to be stored in the troughs\n",
        "def analyze_troughs(signal, pb_dictionary):\n",
        "    total_durations = []\n",
        "    opening_durations = []\n",
        "    closing_durations = []\n",
        "    symmetries = []\n",
        "    opening_velocities = []\n",
        "    closing_velocities = []\n",
        "\n",
        "    for i, trough_time in enumerate(pb_dictionary[\"trough_time\"]):\n",
        "        if (trough_time is None or np.isnan(trough_time) or\n",
        "            i >= len(pb_dictionary[\"stable_points\"]) or\n",
        "            pb_dictionary[\"stable_points\"][i] is None):\n",
        "            print(f\"Skipping index {i}: Invalid trough_time or no stable points.\")\n",
        "            total_durations.append(None)\n",
        "            opening_durations.append(None)\n",
        "            closing_durations.append(None)\n",
        "            symmetries.append(None)\n",
        "            opening_velocities.append(None)\n",
        "            closing_velocities.append(None)\n",
        "            continue\n",
        "\n",
        "        before_idx, after_idx = pb_dictionary[\"stable_points\"][i]\n",
        "        opening_duration = abs(trough_time - before_idx)\n",
        "        closing_duration = abs(after_idx - trough_time)\n",
        "        total_duration = opening_duration + closing_duration\n",
        "\n",
        "        if opening_duration == 0 and closing_duration == 0:\n",
        "            symmetry_percentage = 100\n",
        "        elif opening_duration == 0 or closing_duration == 0:\n",
        "            symmetry_percentage = 0\n",
        "        else:\n",
        "            symmetry_percentage = (min(opening_duration, closing_duration) / max(opening_duration, closing_duration)) * 100\n",
        "\n",
        "        if i < len(pb_dictionary[\"amplitude\"]):\n",
        "            trough_amplitude = pb_dictionary[\"amplitude\"][i]\n",
        "        else:\n",
        "            print(f\"Skipping index {i}: Trough amplitude is missing.\")\n",
        "            total_durations.append(None)\n",
        "            opening_durations.append(None)\n",
        "            closing_durations.append(None)\n",
        "            symmetries.append(None)\n",
        "            opening_velocities.append(None)\n",
        "            closing_velocities.append(None)\n",
        "            continue\n",
        "\n",
        "        stable_before = signal[int(before_idx)]\n",
        "        stable_after  = signal[int(after_idx)]\n",
        "        opening_amp_diff = abs(stable_before - trough_amplitude)\n",
        "        closing_amp_diff = abs(stable_after - trough_amplitude)\n",
        "\n",
        "        opening_velocity = opening_amp_diff / opening_duration if opening_duration > 0 else None\n",
        "        closing_velocity = closing_amp_diff / closing_duration if closing_duration > 0 else None\n",
        "\n",
        "        opening_durations.append(opening_duration)\n",
        "        closing_durations.append(closing_duration)\n",
        "        total_durations.append(total_duration)\n",
        "        symmetries.append(symmetry_percentage)\n",
        "        opening_velocities.append(opening_velocity)\n",
        "        closing_velocities.append(closing_velocity)\n",
        "\n",
        "    return (total_durations, opening_durations, closing_durations, symmetries, opening_velocities, closing_velocities)\n",
        "\n",
        "# This code then actually calcualtes the likeyhood of each trough being a blink\n",
        "def identify_blinks(PB_Dictionary, min_blink_duration, max_blink_duration, blink_trough_threshold, symmetry_threshold, min_duration_percent, max_duration_percent, symmetry_percent, amplitude_percent):\n",
        "    blink_likelyhoods = []\n",
        "    is_blinks = []\n",
        "    total_criteria = 100\n",
        "\n",
        "    for i in range(len(PB_Dictionary[\"trough_time\"])):\n",
        "        criteria_matched = 0\n",
        "        if (\n",
        "            PB_Dictionary[\"duration_total\"][i] is None or\n",
        "            PB_Dictionary[\"symmetry\"][i] is None or\n",
        "            PB_Dictionary[\"amplitude\"][i] is None\n",
        "        ):\n",
        "            blink_likelyhoods.append(0)\n",
        "            is_blinks.append(0)\n",
        "            continue\n",
        "\n",
        "        total_duration = PB_Dictionary[\"duration_total\"][i]\n",
        "        symmetry = PB_Dictionary[\"symmetry\"][i]\n",
        "        amplitude = PB_Dictionary[\"amplitude\"][i]\n",
        "\n",
        "        if total_duration >= min_blink_duration:\n",
        "            criteria_matched += min_duration_percent\n",
        "        if total_duration <= max_blink_duration:\n",
        "            criteria_matched += max_duration_percent\n",
        "        if symmetry <= symmetry_threshold:\n",
        "            criteria_matched += symmetry_percent\n",
        "        if amplitude >= blink_trough_threshold:\n",
        "            criteria_matched += amplitude_percent\n",
        "\n",
        "        blink_likelihood = (criteria_matched / total_criteria) * 100\n",
        "        is_blink = 1 if blink_likelihood >= 70 else 0\n",
        "\n",
        "        blink_likelyhoods.append(blink_likelihood)\n",
        "        is_blinks.append(is_blink)\n",
        "\n",
        "    return blink_likelyhoods, is_blinks\n",
        "\n",
        "# This checks which troughs are good by calculating new values\n",
        "def third_pass(PB_Dictionary, amplitude_threshold, symmetry_threshold, outlier_method, likely_blink_percent, unlikey_blink_percent, likey_blink_factor, unlikey_blink_factor):\n",
        "    tp_indices = [i for i, val in enumerate(PB_Dictionary[\"is_blink\"]) if val == 1]\n",
        "    features = [\"amplitude\", \"symmetry\", \"blink_likelyhood\"]\n",
        "\n",
        "    if not tp_indices:\n",
        "        print(\"No detected blinks to analyze.\")\n",
        "        return PB_Dictionary\n",
        "\n",
        "    feature_matrix = {\n",
        "        feature: [PB_Dictionary[feature][i] for i in tp_indices] for feature in features\n",
        "    }\n",
        "\n",
        "    def detect_outliers(data, method=outlier_method):\n",
        "        data = np.array(data)\n",
        "        if method == \"iqr\":\n",
        "            q1, q3 = np.percentile(data, [25, 75])\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "        elif method == \"zscore\":\n",
        "            mean = np.mean(data)\n",
        "            std_dev = np.std(data)\n",
        "            lower_bound = mean - 3 * std_dev\n",
        "            upper_bound = mean + 3 * std_dev\n",
        "        else:\n",
        "            raise ValueError(\"Invalid outlier method. Use 'iqr' or 'zscore'.\")\n",
        "        return (data >= lower_bound) & (data <= upper_bound)\n",
        "\n",
        "    valid_amplitude_mask = detect_outliers(feature_matrix[\"amplitude\"], method=outlier_method)\n",
        "    valid_symmetry_mask = detect_outliers(feature_matrix[\"symmetry\"], method=outlier_method)\n",
        "    valid_mask = valid_amplitude_mask & valid_symmetry_mask\n",
        "    filtered_amplitude = np.array(feature_matrix[\"amplitude\"])[valid_mask]\n",
        "    filtered_symmetry = np.array(feature_matrix[\"symmetry\"])[valid_mask]\n",
        "\n",
        "    for i in tp_indices:\n",
        "        amplitude = PB_Dictionary[\"amplitude\"][i]\n",
        "        symmetry = PB_Dictionary[\"symmetry\"][i]\n",
        "        likelihood = PB_Dictionary[\"blink_likelyhood\"][i]\n",
        "\n",
        "        likelihood_factor = 1.0\n",
        "        if likelihood >= likely_blink_percent:\n",
        "            likelihood_factor = likey_blink_factor\n",
        "        elif likelihood <= unlikey_blink_percent:\n",
        "            likelihood_factor = unlikey_blink_factor\n",
        "\n",
        "        lower_amplitude = np.mean(filtered_amplitude) - amplitude_threshold * likelihood_factor\n",
        "        upper_amplitude = np.mean(filtered_amplitude) + amplitude_threshold * likelihood_factor\n",
        "        lower_symmetry = np.mean(filtered_symmetry) - symmetry_threshold * likelihood_factor\n",
        "        upper_symmetry = np.mean(filtered_symmetry) + symmetry_threshold * likelihood_factor\n",
        "\n",
        "        if (lower_amplitude <= amplitude <= upper_amplitude) and (lower_symmetry <= symmetry <= upper_symmetry):\n",
        "            PB_Dictionary[\"is_blink\"][i] = 1\n",
        "        else:\n",
        "            PB_Dictionary[\"is_blink\"][i] = 0\n",
        "\n",
        "    return PB_Dictionary\n",
        "\n",
        "# Does a fatigue estimation based on if blinks are increasing, decreasing or staying the same\n",
        "def track_fatigue(PB_Dictionary, time_window, time_increment, fatigue_increment):\n",
        "    fatigue_score = 0\n",
        "    past_avg_duration = 0\n",
        "    past_avg_frequency = 0\n",
        "\n",
        "    if \"duration_total\" not in PB_Dictionary or \"trough_time\" not in PB_Dictionary or \"is_blink\" not in PB_Dictionary:\n",
        "        return fatigue_score\n",
        "\n",
        "    blink_durations = [d for d, b in zip(PB_Dictionary[\"duration_total\"], PB_Dictionary[\"is_blink\"]) if b == 1]\n",
        "    blink_timestamps = [t for t, b in zip(PB_Dictionary[\"trough_time\"], PB_Dictionary[\"is_blink\"]) if b == 1]\n",
        "\n",
        "    if len(blink_durations) < 2:\n",
        "        return fatigue_score\n",
        "\n",
        "    max_time = max(blink_timestamps)\n",
        "\n",
        "    for t in range(time_increment, int(max_time) + 1, time_increment):\n",
        "        recent_durations = [d for time_val, d in zip(blink_timestamps, blink_durations) if t - time_window <= time_val <= t]\n",
        "        avg_duration = np.mean(recent_durations) if recent_durations else past_avg_duration\n",
        "        avg_frequency = len(recent_durations) / time_window if time_window > 0 else 0\n",
        "\n",
        "        if avg_duration > past_avg_duration and avg_frequency > past_avg_frequency:\n",
        "            fatigue_score += fatigue_increment\n",
        "\n",
        "        past_avg_duration = avg_duration\n",
        "        past_avg_frequency = avg_frequency\n",
        "\n",
        "    return fatigue_score\n",
        "\n",
        "# If the files are labeled this code checks if the guesses are correct\n",
        "def validate_blinks(PB_Dictionary, blink_times, proximity_threshold, is_labeled=True):\n",
        "    if 'blink_validation' not in PB_Dictionary or len(PB_Dictionary['blink_validation']) != len(PB_Dictionary['trough_time']):\n",
        "        PB_Dictionary['blink_validation'] = [None] * len(PB_Dictionary['trough_time'])\n",
        "    used_blink_times = set()\n",
        "\n",
        "    for idx, trough_time in enumerate(PB_Dictionary['trough_time']):\n",
        "        candidate_found = False\n",
        "        matched_blink = None\n",
        "        for blink_time in blink_times:\n",
        "            if abs(trough_time - blink_time) <= proximity_threshold:\n",
        "                candidate_found = True\n",
        "                matched_blink = blink_time\n",
        "                break\n",
        "        if candidate_found:\n",
        "            if PB_Dictionary['is_blink'][idx] == 1:\n",
        "                PB_Dictionary['blink_validation'][idx] = 'TP'\n",
        "            else:\n",
        "                PB_Dictionary['blink_validation'][idx] = 'FN'\n",
        "            used_blink_times.add(matched_blink)\n",
        "        else:\n",
        "            PB_Dictionary['blink_validation'][idx] = 'FP'\n",
        "\n",
        "    for blink_time in blink_times:\n",
        "        if blink_time not in used_blink_times and is_labeled:\n",
        "            PB_Dictionary['trough_time'].append(blink_time)\n",
        "            PB_Dictionary['stable_points'].append((0, 0))\n",
        "            PB_Dictionary['amplitude'].append(0)\n",
        "            PB_Dictionary['duration_opening'].append(0)\n",
        "            PB_Dictionary['duration_closing'].append(0)\n",
        "            PB_Dictionary['symmetry'].append(0)\n",
        "            PB_Dictionary['blink_likelyhood'].append(0)\n",
        "            PB_Dictionary['is_blink'].append(0)\n",
        "            PB_Dictionary['duration_total'].append(0)\n",
        "            PB_Dictionary['opening_velocity'].append(0)\n",
        "            PB_Dictionary['closing_velocity'].append(0)\n",
        "            PB_Dictionary['blink_validation'].append('FN')\n",
        "    return PB_Dictionary\n",
        "\n",
        "# Caclulates the final perfomance metrics of the code\n",
        "def calculate_performance_from_dict(PB_Dictionary, total_blinks, global_confusion):\n",
        "    validation = PB_Dictionary[\"blink_validation\"]\n",
        "    tp = validation.count(\"TP\")\n",
        "    fp = validation.count(\"FP\")\n",
        "    fn = validation.count(\"FN\")\n",
        "    tn = total_blinks - (tp + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    accuracy = (tp + tn) / total_blinks if total_blinks > 0 else 0\n",
        "    global_confusion += np.array([[tp, fp], [fn, tn]])\n",
        "    return precision, recall, accuracy, global_confusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphing Functions"
      ],
      "metadata": {
        "id": "OwXaYJEVnjo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the signal and the blinks in order to ensure the system is working correctly\n",
        "def plot_signal(data, time_column, signal, bandpassed_signal, PB_Dictionary, blink_times, channel, table, title, output_folder, SaveOutput, FileLabeled):\n",
        "    save_folder = os.path.join(output_folder, \"MainData\", f\"{table}_{channel}\")\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    fig1 = plt.figure(figsize=(12, 4))\n",
        "    plt.plot(data[time_column], data[channel], label=f'Original Signal ({channel})', color='blue', alpha=0.7)\n",
        "    plt.title('Full Raw Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    if SaveOutput:\n",
        "        plt.savefig(os.path.join(save_folder, \"full_raw_waveform.jpg\"))\n",
        "    plt.show()\n",
        "\n",
        "    fig2 = plt.figure(figsize=(12, 4))\n",
        "    plt.plot(data[time_column], scale_signal(bandpassed_signal), label='Bandpassed Signal', color='orange', alpha=0.8)\n",
        "    plt.title('Full Filtered Waveform')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude (Filtered)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    if SaveOutput:\n",
        "        plt.savefig(os.path.join(save_folder, \"full_filtered_waveform.jpg\"))\n",
        "    plt.show()\n",
        "\n",
        "    fig3 = plt.figure(figsize=(12, 4))\n",
        "    plt.plot(data[time_column], signal, label='Noise-Reduced Signal', color='black')\n",
        "\n",
        "    if PB_Dictionary.get(\"stable_points\"):\n",
        "        sp = [p for p in PB_Dictionary[\"stable_points\"] if p is not None]\n",
        "        xs = [x for pair in sp for x in pair]\n",
        "        ys = [0] * len(xs)\n",
        "        plt.scatter(xs, ys, color='purple', marker='x',\n",
        "                    label='Stable Points', zorder=10)\n",
        "    if FileLabeled:\n",
        "        tp_idx = [i for i, v in enumerate(PB_Dictionary[\"blink_validation\"]) if v == \"TP\"]\n",
        "        fp_idx = [i for i, v in enumerate(PB_Dictionary[\"blink_validation\"]) if v == \"FP\"]\n",
        "        fn_idx = [i for i, v in enumerate(PB_Dictionary[\"blink_validation\"]) if v == \"FN\"]\n",
        "\n",
        "        if tp_idx:\n",
        "            plt.scatter([PB_Dictionary[\"trough_time\"][i] for i in tp_idx], [signal[i] for i in tp_idx], edgecolor='blue', facecolor='green', s=80, label='True Positives', zorder=10)\n",
        "        if fp_idx:\n",
        "            plt.scatter([PB_Dictionary[\"trough_time\"][i] for i in fp_idx], [signal[i] for i in fp_idx], edgecolor='blue', facecolor='red', s=80, label='False Positives', zorder=10)\n",
        "        if fn_idx:\n",
        "            plt.scatter([PB_Dictionary[\"trough_time\"][i] for i in fn_idx], [signal[i] for i in fn_idx], edgecolor='blue', facecolor='blue', s=80, label='False Negatives', zorder=10)\n",
        "    else:\n",
        "        blink_idx = [i for i, b in enumerate(PB_Dictionary[\"is_blink\"]) if b == 1]\n",
        "        not_idx = [i for i, b in enumerate(PB_Dictionary[\"is_blink\"]) if b == 0]\n",
        "\n",
        "        if blink_idx:\n",
        "            plt.scatter([PB_Dictionary[\"trough_time\"][i] for i in blink_idx], [signal[i] for i in blink_idx], edgecolor='blue', facecolor='green', s=80, label='Likely Blink', zorder=10)\n",
        "        if not_idx:\n",
        "            plt.scatter([PB_Dictionary[\"trough_time\"][i] for i in not_idx], [signal[i] for i in not_idx], edgecolor='black', facecolor='orange', s=80, label='Not A Blink Trough', zorder=10)\n",
        "\n",
        "    if blink_times is not None and len(blink_times) > 0:\n",
        "        for i, bt in enumerate(blink_times):\n",
        "            plt.axvline(x=bt, color='green', linestyle='--', label='Known Blink' if i == 0 else \"\", zorder=1)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude (Noise-Reduced)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    if SaveOutput:\n",
        "        plt.savefig(os.path.join(save_folder, \"full_simplified_waveform.jpg\"))\n",
        "    plt.show()\n",
        "\n",
        "def analyze_blink_similarity(PB_Dictionary):\n",
        "    tp_indices = [i for i, val in enumerate(PB_Dictionary[\"blink_validation\"]) if val in [\"TP\", \"FN\", \"FP\"]]\n",
        "    features = [\"amplitude\", \"duration_total\", \"symmetry\"]\n",
        "\n",
        "    if not tp_indices:\n",
        "        print(\"No true positives detected. Skipping analysis.\")\n",
        "        return None, pd.DataFrame(columns=[\"Feature\", \"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Range\"])\n",
        "\n",
        "    feature_matrix = np.array([[PB_Dictionary[feature][i] for feature in features] for i in tp_indices])\n",
        "    feature_matrix = np.nan_to_num(feature_matrix)\n",
        "\n",
        "    if feature_matrix.ndim == 1:\n",
        "        feature_matrix = feature_matrix.reshape(-1, len(features))\n",
        "\n",
        "    stats = {\"Feature\": [], \"Mean\": [], \"Std Dev\": [], \"Min\": [], \"Max\": [], \"Range\": []}\n",
        "    for i, feature in enumerate(features):\n",
        "        column = feature_matrix[:, i]\n",
        "        stats[\"Feature\"].append(feature.capitalize())\n",
        "        stats[\"Mean\"].append(np.mean(column))\n",
        "        stats[\"Std Dev\"].append(np.std(column))\n",
        "        stats[\"Min\"].append(np.min(column))\n",
        "        stats[\"Max\"].append(np.max(column))\n",
        "        stats[\"Range\"].append(np.ptp(column))\n",
        "\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "\n",
        "    print(\"\\nCompare Blink Critera:\")\n",
        "    print(stats_df.to_string(index=False, float_format=\"{:.2f}\".format))\n",
        "\n",
        "    similarity_matrix = cosine_similarity(feature_matrix)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    x = feature_matrix[:, 0]\n",
        "    y = feature_matrix[:, 1]\n",
        "    z = feature_matrix[:, 2]\n",
        "\n",
        "    edge_colors = { \"TP\": \"green\", \"FP\": \"red\", \"FN\": \"blue\"}\n",
        "\n",
        "    scatter = ax.scatter(x, y, z, c=z, cmap=\"viridis\", edgecolors=[edge_colors[PB_Dictionary[\"blink_validation\"][idx]] for idx in tp_indices], s=100, linewidth=2)\n",
        "\n",
        "    fig.colorbar(scatter, ax=ax, label=\"Symmetry (Color Gradient)\")\n",
        "    ax.set_title(\"Blink Critera Similarity\")\n",
        "    ax.set_xlabel(\"Amplitude\")\n",
        "    ax.set_ylabel(\"Duration Total\")\n",
        "    ax.set_zlabel(\"Symmetry\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return similarity_matrix, stats_df\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "def plot_confusion_matrix(conf_matrix):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Blink', 'Predicted Not Blink'], yticklabels=['Actual Blink', 'Actual Not Blink'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    tp, fp, fn, tn = conf_matrix[0, 0], conf_matrix[0, 1], conf_matrix[1, 0], conf_matrix[1, 1]\n",
        "    final_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    final_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    final_accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0\n",
        "\n",
        "    print(\"\\nFinal Performance Metrics (Cumulative):\")\n",
        "    print(f\"Precision: {final_precision:.2f}\")\n",
        "    print(f\"Recall: {final_recall:.2f}\")\n",
        "    print(f\"Accuracy: {final_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "jp84-y-6njN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}